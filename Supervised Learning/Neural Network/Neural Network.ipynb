{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "----\n",
    "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure shows a one hidden layer MLP with scalar output.\n",
    " <img src=\"https://sebastianraschka.com/images/faq/visual-backpropagation/backpropagation.png\">\n",
    " \n",
    " ----\n",
    " \n",
    "The advantages of Multi-layer Perceptron are:\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "The disadvantages of Multi-layer Perceptron (MLP) include:\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n8LogKtg4MSG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Dropout, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ-KGPnj4StB",
    "outputId": "bb4813d7-13c4-4d14-877c-ad073190c579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Data Split\n",
    "(train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, stratify=train_Y, train_size=7/8)\n",
    "train_Y = to_categorical(train_Y) # One-hot\n",
    "valid_Y = to_categorical(valid_Y)\n",
    "test_Y = to_categorical(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Algorithm implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmF55eC14V--",
    "outputId": "5136199d-1812-4638-deba-0403291549c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1\n",
      "# neurons in the layers: [100, 178, 147]\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 00032: early stopping\n",
      "Validation loss:0.37043\n",
      "Updating best yet model...\n",
      "\n",
      "Iteration: 2\n",
      "# neurons in the layers: [252, 143, 118]\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 00034: early stopping\n",
      "Validation loss:0.36231\n",
      "Updating best yet model...\n",
      "\n",
      "Iteration: 3\n",
      "# neurons in the layers: [227, 193, 21]\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 00026: early stopping\n",
      "Validation loss:2.30259\n",
      "\n",
      "Iteration: 4\n",
      "# neurons in the layers: [138, 18, 175]\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 00034: early stopping\n",
      "Validation loss:2.30260\n",
      "\n",
      "Iteration: 5\n",
      "# neurons in the layers: [105, 63, 172]\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 00045: early stopping\n",
      "Validation loss:0.38912\n",
      "\n",
      "Iteration: 6\n",
      "# neurons in the layers: [164, 137, 161]\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 00035: early stopping\n",
      "Validation loss:0.35605\n",
      "Updating best yet model...\n",
      "\n",
      "Iteration: 7\n",
      "# neurons in the layers: [177, 135, 84]\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Epoch 00043: early stopping\n",
      "Validation loss:0.34268\n",
      "Updating best yet model...\n",
      "\n",
      "Iteration: 8\n",
      "# neurons in the layers: [172, 164, 154]\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 00040: early stopping\n",
      "Validation loss:0.34986\n",
      "\n",
      "Iteration: 9\n",
      "# neurons in the layers: [145, 241, 196]\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 00035: early stopping\n",
      "Validation loss:0.37041\n",
      "\n",
      "Iteration: 10\n",
      "# neurons in the layers: [125, 140, 152]\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 00033: early stopping\n",
      "Validation loss:0.36368\n"
     ]
    }
   ],
   "source": [
    "def MLP(input_shape, neuronsList, opt=Adam(learning_rate=0.001)):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Flatten()(inputs)\n",
    "    for i in range(len(neuronsList)):\n",
    "        layer = Dense(neuronsList[i], activation='relu')(layer)\n",
    "    \n",
    "    outputs = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(learning_rate=0.001)\n",
    "#Early stopping to avoid overfitting\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   verbose=1,\n",
    "                   patience=20,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "# Random search for hyperparameters tuning\n",
    "nLayers = 3\n",
    "nIter = 10 # Number of iterations in random search\n",
    "BestValidLoss = 1. # Best yet validation loss\n",
    "\n",
    "for i in range(nIter):\n",
    "    print('\\nIteration:', i+1)\n",
    "    neuronsList = []\n",
    "\n",
    "    # Randomly choose integers between 16 and 256 for nLayers layers\n",
    "    for j in range(nLayers):\n",
    "        neuronsList.append(np.random.randint(low=16, high=257))\n",
    "\n",
    "    print('# neurons in the layers:', neuronsList)\n",
    "  \n",
    "    mlp = MLP(input_shape=train_X[0].shape, neuronsList=neuronsList, opt=adam)\n",
    "    history = mlp.fit(train_X, train_Y, epochs=80, batch_size=64, validation_data=(valid_X, valid_Y), verbose=0, callbacks=[es])\n",
    "\n",
    "    # validation\n",
    "    validLoss, validAcc = mlp.evaluate(valid_X, valid_Y, verbose=0)\n",
    "\n",
    "    print('Validation loss:{:0.5f}'.format(validLoss))\n",
    "\n",
    "    # Store best model and history\n",
    "    if validLoss < BestValidLoss:\n",
    "        print('Updating best yet model...')\n",
    "        BestValidLoss = validLoss\n",
    "        BestModel = mlp\n",
    "        History = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfKH7ojj4w3F",
    "outputId": "1bb0bbe5-41d8-4eed-e17d-ccc880c7b2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3680 - accuracy: 0.8751\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "test_a = BestModel.evaluate(test_X,test_Y,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After ten iterations, the accuracy of the model reached 87.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
